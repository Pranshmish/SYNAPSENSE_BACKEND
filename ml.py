"""
Hybrid Anomaly Detection + Supervised Fallback ML Module
Primary: Isolation Forest (one-class HOME learner)
Secondary: LightGBM (binary fallback for synthetic testing)

Fixes overfitting/underfitting issues:
- Reduced model complexity
- Robust scaling
- Cross-validation with session splits
- Feature dropout for regularization
"""

import os
import joblib
import numpy as np
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from typing import List, Dict, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Try to import LightGBM, fallback to RandomForest if not available
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("[ML] LightGBM not installed, using RandomForest fallback")

from features import FEATURE_NAMES, get_feature_vector

MODELS_DIR = "models"
IF_MODEL_PATH = os.path.join(MODELS_DIR, "home_detector_if.pkl")
SCALER_PATH = os.path.join(MODELS_DIR, "feature_scaler.pkl")
METADATA_PATH = os.path.join(MODELS_DIR, "model_metadata.pkl")
SVM_MODEL_PATH = os.path.join(MODELS_DIR, "home_detector_svm.pkl")
LGBM_MODEL_PATH = os.path.join(MODELS_DIR, "binary_fallback_lgbm.pkl")

# Labels
LABEL_HOME = "HOME"
LABEL_INTRUDER = "INTRUDER"

# Improved Isolation Forest parameters (fixes overfitting)
IF_PARAMS = {
    'n_estimators': 300,
    'max_samples': 'auto',
    'contamination': 0.015,  # Reduced from 0.02 - less aggressive outlier detection
    'bootstrap': True,
    'random_state': 42,
    'n_jobs': -1,
    'max_features': 0.8  # Feature dropout to prevent overfitting
}

# LightGBM parameters (for binary fallback testing)
LGBM_PARAMS = {
    'num_leaves': 12,  # Reduced complexity
    'min_data_in_leaf': 10,
    'max_depth': 4,  # Shallow trees prevent overfitting
    'learning_rate': 0.05,
    'n_estimators': 120,
    'feature_fraction': 0.85,  # Feature dropout
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'random_state': 42
}


class AnomalyDetector:
    """
    Hybrid One-Class Anomaly Detection for HOME footstep recognition.
    
    Primary: IsolationForest - trains ONLY on HOME samples
    Secondary: LightGBM/RandomForest - binary fallback for diagnostic tests
    
    Fixes overfitting:
    - Uses RobustScaler (handles outliers better)
    - Feature dropout during training
    - Reduced model complexity
    - Cross-validation with session awareness
    """
    
    def __init__(self):
        self._init_dirs()
        self.isolation_forest: Optional[IsolationForest] = None
        self.one_class_svm: Optional[OneClassSVM] = None
        self.binary_model = None  # LightGBM or RandomForest fallback
        self.scaler: Optional[RobustScaler] = None
        self.metadata: Dict[str, Any] = {}
        self.is_trained = False
        self.anomaly_threshold = 0.0
        self.confidence_calibration = {'mean': 0, 'std': 1}
        self.load_models()
        
    def _init_dirs(self):
        """Ensure model directory exists."""
        os.makedirs(MODELS_DIR, exist_ok=True)
        
    def load_models(self) -> bool:
        """Load trained models from disk if available."""
        if (os.path.exists(IF_MODEL_PATH) and 
            os.path.exists(SCALER_PATH) and 
            os.path.exists(METADATA_PATH)):
            try:
                self.isolation_forest = joblib.load(IF_MODEL_PATH)
                self.scaler = joblib.load(SCALER_PATH)
                self.metadata = joblib.load(METADATA_PATH)
                self.anomaly_threshold = self.metadata.get('anomaly_threshold', 0.0)
                self.confidence_calibration = self.metadata.get('confidence_calibration', {'mean': 0, 'std': 1})
                
                # Load optional models
                if os.path.exists(SVM_MODEL_PATH):
                    self.one_class_svm = joblib.load(SVM_MODEL_PATH)
                if os.path.exists(LGBM_MODEL_PATH):
                    self.binary_model = joblib.load(LGBM_MODEL_PATH)
                    
                self.is_trained = True
                print(f"[ML] Hybrid detector loaded. HOME samples: {self.metadata.get('home_samples', 'N/A')}")
                return True
            except Exception as e:
                print(f"[ML] Error loading models: {e}")
                self.is_trained = False
                return False
        else:
            print("[ML] No trained models found. Training required.")
            return False
            
    def save_models(self):
        """Save trained models to disk."""
        try:
            joblib.dump(self.isolation_forest, IF_MODEL_PATH)
            joblib.dump(self.scaler, SCALER_PATH)
            joblib.dump(self.metadata, METADATA_PATH)
            
            if self.one_class_svm is not None:
                joblib.dump(self.one_class_svm, SVM_MODEL_PATH)
            if self.binary_model is not None:
                joblib.dump(self.binary_model, LGBM_MODEL_PATH)
                
            print("[ML] Models saved successfully.")
        except Exception as e:
            print(f"[ML] Error saving models: {e}")
            
    def reset(self):
        """Reset/delete all trained models."""
        self.isolation_forest = None
        self.one_class_svm = None
        self.binary_model = None
        self.scaler = None
        self.metadata = {}
        self.is_trained = False
        self.anomaly_threshold = 0.0
        
        for path in [IF_MODEL_PATH, SCALER_PATH, METADATA_PATH, SVM_MODEL_PATH, LGBM_MODEL_PATH]:
            if os.path.exists(path):
                try:
                    os.remove(path)
                except Exception as e:
                    print(f"[ML] Error removing {path}: {e}")
                    
        print("[ML] Models reset.")
        
    def prepare_features(self, data: List[Dict[str, float]], apply_noise: bool = False) -> np.ndarray:
        """
        Convert list of feature dictionaries to numpy array.
        Optionally applies small gaussian noise for augmentation.
        """
        X = []
        for item in data:
            if isinstance(item, dict):
                row = [float(item.get(name, 0.0)) for name in FEATURE_NAMES]
            else:
                row = [float(v) for v in item]
            X.append(row)
        
        X = np.array(X, dtype=np.float64)
        
        # Apply small gaussian noise for augmentation/regularization
        if apply_noise and len(X) > 0:
            noise_scale = 0.01 * np.std(X, axis=0)
            noise = np.random.normal(0, noise_scale, X.shape)
            X = X + noise
            
        return X
    
    def _apply_gain_normalization(self, X: np.ndarray) -> np.ndarray:
        """
        Apply adaptive gain normalization to handle variable signal amplitudes.
        Fixes issues with low/medium vibration synthetic tests.
        """
        # Find amplitude-related features and normalize
        rms_idx = FEATURE_NAMES.index('stat_rms') if 'stat_rms' in FEATURE_NAMES else None
        energy_idx = FEATURE_NAMES.index('stat_energy') if 'stat_energy' in FEATURE_NAMES else None
        
        X_normalized = X.copy()
        
        # Per-sample gain normalization based on RMS
        if rms_idx is not None:
            rms_values = X[:, rms_idx].reshape(-1, 1)
            rms_values = np.clip(rms_values, 0.01, None)  # Prevent division by zero
            
            # Normalize amplitude-dependent features
            amplitude_features = ['stat_max', 'stat_min', 'stat_range', 'stat_rms', 
                                  'stat_energy', 'stat_power', 'stat_peak_mean']
            for feat in amplitude_features:
                if feat in FEATURE_NAMES:
                    idx = FEATURE_NAMES.index(feat)
                    # Normalize by RMS to make features amplitude-invariant
                    X_normalized[:, idx] = X[:, idx] / (rms_values.flatten() + 1e-6)
                    
        return X_normalized
        
    def train(self, data: List[Dict[str, float]], labels: List[str]) -> Dict[str, Any]:
        """
        Train the hybrid anomaly detector.
        
        Primary (IF): Trains ONLY on HOME samples
        Secondary (LGBM): Optional binary classifier for testing
        
        Includes overfitting prevention:
        - RobustScaler
        - Feature dropout
        - Cross-validation
        """
        # Separate HOME and INTRUDER data
        home_data = []
        intruder_data = []
        
        for item, label in zip(data, labels):
            label_upper = label.upper()
            if label_upper == LABEL_HOME or label_upper == "HOME_SAMPLE":
                home_data.append(item)
            else:
                intruder_data.append(item)
                
        home_count = len(home_data)
        intruder_count = len(intruder_data)
        
        # Validate - need enough HOME samples
        if home_count < 5:
            return {
                "success": False,
                "error": f"Not enough HOME samples. Need at least 5, got {home_count}.",
                "samples_provided": len(data),
                "home_samples": home_count
            }
            
        print(f"[ML] Training on {home_count} HOME samples (+ {intruder_count} INTRUDER for fallback)")
            
        # Prepare feature matrices
        X_home = self.prepare_features(home_data)
        
        # Handle NaN/Inf values
        X_home = np.nan_to_num(X_home, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Apply gain normalization for variable amplitude handling
        X_home = self._apply_gain_normalization(X_home)
        
        # Initialize RobustScaler (handles outliers better than StandardScaler)
        self.scaler = RobustScaler(quantile_range=(10, 90))
        X_home_scaled = self.scaler.fit_transform(X_home)
        
        # ============== PRIMARY: ISOLATION FOREST ==============
        self.isolation_forest = IsolationForest(**IF_PARAMS)
        self.isolation_forest.fit(X_home_scaled)
        
        # Calculate anomaly scores for threshold calibration
        home_scores = -self.isolation_forest.decision_function(X_home_scaled)
        
        # Calibrate threshold: mean + 1.5*std (less aggressive than 2*std)
        score_mean = float(np.mean(home_scores))
        score_std = float(np.std(home_scores))
        self.anomaly_threshold = score_mean + 1.5 * score_std
        
        # Store calibration for confidence calculation
        self.confidence_calibration = {
            'mean': score_mean,
            'std': score_std,
            'threshold': self.anomaly_threshold
        }
        
        # Evaluate on training data
        train_predictions = self.isolation_forest.predict(X_home_scaled)
        home_correctly_identified = int(np.sum(train_predictions == 1))
        training_accuracy = home_correctly_identified / len(train_predictions) * 100
        
        # ============== CROSS-VALIDATION ==============
        cv_scores = []
        n_splits = min(5, home_count)
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        
        for train_idx, val_idx in kf.split(X_home_scaled):
            X_train_cv = X_home_scaled[train_idx]
            X_val_cv = X_home_scaled[val_idx]
            
            if_cv = IsolationForest(
                n_estimators=150,
                contamination=0.015,
                max_features=0.8,
                random_state=42,
                n_jobs=-1
            )
            if_cv.fit(X_train_cv)
            
            val_pred = if_cv.predict(X_val_cv)
            cv_accuracy = np.mean(val_pred == 1) * 100
            cv_scores.append(cv_accuracy)
            
        cv_mean = float(np.mean(cv_scores))
        cv_std = float(np.std(cv_scores))
        
        # ============== SECONDARY: OneClassSVM ==============
        if home_count >= 8:
            try:
                self.one_class_svm = OneClassSVM(
                    kernel='rbf',
                    gamma='scale',
                    nu=0.03  # Lower nu for less aggressive outlier detection
                )
                self.one_class_svm.fit(X_home_scaled)
                print("[ML] OneClassSVM trained as secondary detector.")
            except Exception as e:
                print(f"[ML] OneClassSVM training skipped: {e}")
                self.one_class_svm = None
        
        # ============== BINARY FALLBACK: LightGBM/RandomForest ==============
        # Only train if we have INTRUDER samples (for synthetic testing)
        binary_metrics = None
        if intruder_count >= 3:
            try:
                X_intruder = self.prepare_features(intruder_data)
                X_intruder = np.nan_to_num(X_intruder, nan=0.0, posinf=0.0, neginf=0.0)
                X_intruder = self._apply_gain_normalization(X_intruder)
                X_intruder_scaled = self.scaler.transform(X_intruder)
                
                # Combine for binary classification
                X_binary = np.vstack([X_home_scaled, X_intruder_scaled])
                y_binary = np.array([0] * home_count + [1] * intruder_count)
                
                if HAS_LIGHTGBM:
                    self.binary_model = lgb.LGBMClassifier(**LGBM_PARAMS)
                else:
                    self.binary_model = RandomForestClassifier(
                        n_estimators=120,
                        max_depth=4,
                        min_samples_leaf=10,
                        max_features=0.8,
                        random_state=42,
                        n_jobs=-1
                    )
                
                # Cross-validate binary model
                binary_cv_scores = []
                skf = StratifiedKFold(n_splits=min(3, min(home_count, intruder_count)), 
                                      shuffle=True, random_state=42)
                
                for train_idx, val_idx in skf.split(X_binary, y_binary):
                    X_train_b, X_val_b = X_binary[train_idx], X_binary[val_idx]
                    y_train_b, y_val_b = y_binary[train_idx], y_binary[val_idx]
                    
                    if HAS_LIGHTGBM:
                        model_cv = lgb.LGBMClassifier(**LGBM_PARAMS)
                    else:
                        model_cv = RandomForestClassifier(
                            n_estimators=120, max_depth=4, 
                            min_samples_leaf=10, random_state=42
                        )
                    model_cv.fit(X_train_b, y_train_b)
                    y_pred = model_cv.predict(X_val_b)
                    binary_cv_scores.append(accuracy_score(y_val_b, y_pred))
                
                # Train final binary model on all data
                self.binary_model.fit(X_binary, y_binary)
                
                binary_metrics = {
                    'cv_accuracy': f"{np.mean(binary_cv_scores)*100:.1f}%",
                    'model_type': 'LightGBM' if HAS_LIGHTGBM else 'RandomForest'
                }
                print(f"[ML] Binary fallback trained ({binary_metrics['model_type']})")
                
            except Exception as e:
                print(f"[ML] Binary fallback training failed: {e}")
                self.binary_model = None
        
        # ============== FEATURE IMPORTANCE ==============
        feature_importance = np.var(X_home_scaled, axis=0)
        feature_importance = feature_importance / (np.sum(feature_importance) + 1e-10)
        top_features = sorted(
            zip(FEATURE_NAMES, feature_importance), 
            key=lambda x: x[1], 
            reverse=True
        )[:10]
        
        # ============== STORE METADATA ==============
        self.metadata = {
            "home_samples": home_count,
            "intruder_samples": intruder_count,
            "training_accuracy": training_accuracy,
            "cv_accuracy_mean": cv_mean,
            "cv_accuracy_std": cv_std,
            "anomaly_threshold": self.anomaly_threshold,
            "confidence_calibration": self.confidence_calibration,
            "top_features": top_features,
            "model_type": "Hybrid (IF + SVM + Binary Fallback)",
            "has_svm_backup": self.one_class_svm is not None,
            "has_binary_fallback": self.binary_model is not None,
            "scaler_type": "RobustScaler",
            "if_params": IF_PARAMS
        }
        
        # Save models
        self.is_trained = True
        self.save_models()
        
        return {
            "success": True,
            "model_type": "Hybrid Anomaly Detection (IF + Fallback)",
            "metrics": {
                "training_accuracy": round(training_accuracy, 2),
                "cv_accuracy": f"{cv_mean:.1f}% Â± {cv_std:.1f}%",
                "home_recognition_rate": f"{home_correctly_identified}/{home_count}",
                "anomaly_threshold": round(self.anomaly_threshold, 4)
            },
            "dataset": {
                "home_samples_used": home_count,
                "intruder_samples": intruder_count,
                "total_provided": len(data)
            },
            "calibration": {
                "score_mean": round(score_mean, 4),
                "score_std": round(score_std, 4),
                "threshold": round(self.anomaly_threshold, 4)
            },
            "top_features": [{"name": name, "importance": float(imp)} for name, imp in top_features[:5]],
            "binary_fallback": binary_metrics,
            "overfitting_prevention": {
                "scaler": "RobustScaler (outlier-resistant)",
                "feature_dropout": "15%",
                "cv_folds": n_splits,
                "model_complexity": "Reduced (max_depth=4)"
            },
            "note": "Hybrid model: IF for one-class, binary fallback for testing"
        }
        
    def predict(self, features: List[float]) -> Dict[str, Any]:
        """
        Predict if footstep is HOME (normal) or INTRUDER (anomaly).
        
        Returns comprehensive result with:
        - Anomaly score and confidence
        - IF and SVM predictions
        - Binary model probability (if available)
        - Confidence band (high/medium/low)
        """
        if not self.is_trained or self.isolation_forest is None or self.scaler is None:
            return {
                "success": False,
                "error": "Model not trained. Please collect HOME samples and train.",
                "prediction": None
            }
            
        try:
            # Reshape and prepare features
            X = np.array(features, dtype=np.float64).reshape(1, -1)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Apply gain normalization
            X = self._apply_gain_normalization(X)
            
            # Scale features
            X_scaled = self.scaler.transform(X)
            
            # ============== ISOLATION FOREST PREDICTION ==============
            if_prediction = self.isolation_forest.predict(X_scaled)[0]
            raw_score = self.isolation_forest.decision_function(X_scaled)[0]
            anomaly_score = -raw_score  # Higher = more anomalous
            
            # ============== CALCULATE CONFIDENCE ==============
            cal = self.confidence_calibration
            z_score = (anomaly_score - cal['mean']) / (cal['std'] + 1e-6)
            
            # Sigmoid-based confidence
            if if_prediction == 1:  # HOME
                confidence = float(1 / (1 + np.exp(z_score * 0.8)))  # Dampened sigmoid
            else:  # INTRUDER
                confidence = float(1 / (1 + np.exp(-z_score * 0.8)))
            
            confidence = max(0.45, min(0.98, confidence))
            
            # ============== SVM CROSS-CHECK ==============
            svm_prediction = None
            svm_agrees = None
            if self.one_class_svm is not None:
                svm_prediction = self.one_class_svm.predict(X_scaled)[0]
                svm_agrees = (svm_prediction == 1 and if_prediction == 1) or \
                            (svm_prediction == -1 and if_prediction == -1)
                            
            # ============== BINARY FALLBACK ==============
            binary_prob = None
            binary_prediction = None
            if self.binary_model is not None:
                try:
                    binary_prob = self.binary_model.predict_proba(X_scaled)[0]
                    binary_prediction = self.binary_model.predict(X_scaled)[0]
                except:
                    pass
            
            # ============== FINAL DECISION ==============
            # Use IF as primary, with SVM confirmation for uncertain cases
            prediction = LABEL_HOME if if_prediction == 1 else LABEL_INTRUDER
            is_intruder = prediction == LABEL_INTRUDER
            
            # Adjust confidence based on model agreement
            if svm_agrees is not None:
                if svm_agrees:
                    confidence = min(0.98, confidence + 0.05)  # Boost if models agree
                else:
                    confidence = max(0.45, confidence - 0.1)  # Reduce if disagreement
            
            # ============== CONFIDENCE BAND ==============
            if confidence >= 0.8:
                confidence_band = "high"
            elif confidence >= 0.6:
                confidence_band = "medium"
            else:
                confidence_band = "low"
            
            # ============== PROBABILITIES ==============
            prob_home = 1 - confidence if is_intruder else confidence
            prob_intruder = confidence if is_intruder else 1 - confidence
            
            return {
                "success": True,
                "prediction": prediction,
                "is_intruder": is_intruder,
                "confidence": round(confidence, 3),
                "confidence_band": confidence_band,
                "anomaly_score": round(float(anomaly_score), 4),
                "threshold": round(float(self.anomaly_threshold), 4),
                "raw_score": round(float(raw_score), 4),
                "z_score": round(float(z_score), 4),
                "if_prediction": "HOME" if if_prediction == 1 else "INTRUDER",
                "svm_prediction": "HOME" if svm_prediction == 1 else ("INTRUDER" if svm_prediction == -1 else None),
                "svm_agrees": svm_agrees,
                "binary_prediction": "HOME" if binary_prediction == 0 else ("INTRUDER" if binary_prediction == 1 else None),
                "binary_probabilities": {
                    "HOME": round(float(binary_prob[0]), 3) if binary_prob is not None else None,
                    "INTRUDER": round(float(binary_prob[1]), 3) if binary_prob is not None else None
                } if binary_prob is not None else None,
                "probabilities": {
                    LABEL_HOME: round(prob_home, 3),
                    LABEL_INTRUDER: round(prob_intruder, 3)
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "prediction": None
            }
            
    def get_status(self) -> Dict[str, Any]:
        """Get current model status and metadata."""
        if not self.is_trained:
            return {
                "trained": False,
                "message": "No model trained. Collect HOME samples and train."
            }
            
        return {
            "trained": True,
            "model_type": "Hybrid Anomaly Detection",
            "home_samples": self.metadata.get("home_samples", 0),
            "intruder_samples": self.metadata.get("intruder_samples", 0),
            "training_accuracy": self.metadata.get("training_accuracy", 0),
            "cv_accuracy": self.metadata.get("cv_accuracy_mean", 0),
            "anomaly_threshold": self.metadata.get("anomaly_threshold", 0),
            "has_svm_backup": self.metadata.get("has_svm_backup", False),
            "has_binary_fallback": self.metadata.get("has_binary_fallback", False),
            "scaler_type": self.metadata.get("scaler_type", "Unknown"),
            "top_features": self.metadata.get("top_features", [])[:5],
            "note": "Hybrid: IF for one-class anomaly, binary fallback for testing"
        }


# Backward compatibility alias
class BinaryClassifier(AnomalyDetector):
    """Backward compatibility wrapper - redirects to AnomalyDetector."""
    pass


# Global instance for use by API
ml_manager = AnomalyDetector()


# Legacy compatibility functions
def train_model(data: List[Dict[str, float]], labels: List[str]) -> Dict[str, Any]:
    """Train the model (legacy wrapper)."""
    return ml_manager.train(data, labels)


def predict_footstep(features: List[float]) -> Dict[str, Any]:
    """Predict footstep (legacy wrapper)."""
    return ml_manager.predict(features)


def get_model_status() -> Dict[str, Any]:
    """Get model status (legacy wrapper)."""
    return ml_manager.get_status()


def reset_model():
    """Reset model (legacy wrapper)."""
    ml_manager.reset()
